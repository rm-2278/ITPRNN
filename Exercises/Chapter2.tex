\documentclass[12pt, a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{titlesec}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage[bottom=3cm]{geometry}
\usepackage[hidelinks]{hyperref}
\tcbuselibrary{breakable}
\tcbuselibrary{skins}

\newtcolorbox{prob}[1]{colback=gray!5!white, colframe=gray!75!black, 
title=\textbf{Exercise #1}}

\newtcolorbox{sol}{
    breakable,
    colback=white,      % Background matches page
    colframe=white,     % Frame matches page (invisible)
    frame hidden,       % Hides the border line
    left=3mm, right=3mm,% MATCHES the Question box padding
    boxrule=0mm,        % No border width
    top=0mm, bottom=0mm,% Tight vertical spacing
    parbox=false,       % Allows paragraphs to break normally
    before upper={\textbf{Solution:}\par\medskip} % Automatically adds "Solution:" title
}

\begin{document}
\begin{prob}{2.2}
Are the random variables X and Y in the joint ensemble
of figure 2.2 independent?
\end{prob}
\begin{sol}
No, $P(x, y) \neq P(x)P(y)$ since each row or column are not proportional to each other.
\end{sol}
\bigskip

\begin{prob}{2.4}
An urn contains $K$ balls, of which $B$ are black and $W = K - B$ are white. Fred draws a ball at random from the urn and replaces
it, N times.
\begin{enumerate}
    \item[(a)] What is the probability distribution of the number of times a black
ball is drawn, $n_B$?
    \item[(b)] What is the expectation of $n_B$? What is the variance of $n_B$? What
is the standard deviation of $n_B$? Give numerical answers for the
cases $N = 5$ and $N = 400$, when $B = 2$ and $K = 10$.
\end{enumerate}
\end{prob}
\begin{sol}
\begin{enumerate}
\item[(a)] $P(n_B) = \binom{N}{n_B} (\frac{B}{K})^{n_B} (\frac{K - B}{K})^{N - n_B}$
\item[(b)] $\frac{B}{K} = \frac{1}{5}$, $\frac{K-B}{K} = \frac{4}{5}$ so the distribution is B($n$, $\frac{1}{5}$). \\[2ex]
Hence, \, $\operatorname{E}[n_b] = \frac{1}{5}n$, $\operatorname{Var}[n_B] = \frac{1}{5}n(1-\frac{1}{5})$, $\operatorname{Std}[n_B] = \sqrt{\operatorname{Var}[n_B]}$ \\[2ex]
For n = 5: \, $\operatorname{E}[n_b] = 1$, $\operatorname{Var}[n_B] = \frac{4}{5}$, $\operatorname{Std}[n_B] = \sqrt{\frac{4}{5}}$ \\[2ex]
For n = 400: \, $\operatorname{E}[n_b] = 80$, $\operatorname{Var}[n_B] = 64$, $\operatorname{Std}[n_B] = 8$ \\
\end{enumerate}
\end{sol}
\bigskip

\begin{prob}{2.5}
    An urn contains $K$ balls, of which $B$ are black and $W = K - B$ are white. We define the fraction $f_B \equiv \frac{B}{K}$. Fred draws $N$
times from the urn, exactly as in exercise 2.4, obtaining $n_B$ blacks, and
computes the quantity
\begin{equation}
z = \frac{(n_B - f_BN)^2}{Nf_B(1 - f_B)}. \tag{2.19}
\end{equation}
What is the expectation of $z$? In the case $N = 5$ and $f_B = \frac{1}{5}$, what
is the probability distribution of $z$? What is the probability that $z < 1$?
[Hint: compare $z$ with the quantities computed in the previous exercise.]
\end{prob}
\begin{sol}
\begin{align*}
    \operatorname{E}[z] &= \frac{1}{Nf_b(1-f_B)}(\operatorname{E}[n_B^2] - 2\operatorname{E}[f_B n_B N] + \operatorname{E}[f_B^2 N^2])
    &= \frac{1}{Nf(1-f)}(Nf(1-f) + N^2f^2 - 2N^2 f^2 + N^2f^2)
    &= 1
\end{align*}
With $N = 5$ and $f_B = \frac{1}{5}$,
\begin{align*}
    z &= \frac{(n_B - 1)^2}{\frac{4}{5}}
\end{align*}
so
\begin{center}
\begin{tabular}{|c|cccccc|}
    \hline
    $n_B$ & 0 & 1 & 2 & 3 & 4 & 5 \\
    \hline
    $z$ &\rule[-12pt]{0pt}{30pt} $\frac{5}{4}$ & 0 & $\frac{5}{4}$ & $5$ & $\frac{45}{4}$ & 20 \\
    \hline
\end{tabular}
\end{center}

Hence, 
$$P(z < 1) = P(n_B = 1) = \binom{5}{1} \cdot \left(\frac{1}{5}\right) \left(\frac{4}{5}\right)^4 = \frac{256}{625} = 0.4096$$
\end{sol}
\bigskip

\begin{prob}{2.8}
Assuming a uniform prior on $f_H$, $P(f_H) = 1$, solve the problem
posed in example 2.7 (p.30). Sketch the posterior distribution of $f_H$ and
compute the probability that the $N + 1$th outcome will be a head, for
\begin{enumerate}
\item[(a)] $N = 3$ and $n_H = 0$;
\item[(b)] $N = 3$ and $n_H = 2$;
\item[(c)] $N = 10$ and $n_H = 3$;
\item[(d)] $N = 300$ and $n_H = 29$.
\end{enumerate}

\end{prob}
\begin{sol}
\begin{align*}
    P(f_H | n_H, N) &= \frac{P(n_H | f_H, N) P(f_H)}{P(n_H | N)}
    &= \frac{P(n_H | f_H, N)}{P(n_H | N)} = \frac{\binom{N}{n_H} f_H^{n_H} (1-f_H)^{N-n_H}}{P(n_H | N)}
\end{align*}
Now,
\begin{align*}
    & \int P(f_H | n_H, N) df_H = 1 \\
    \iff & \frac{\binom{N}{n_H}}{P(n_H | N)} \int_{1}^{0} f_H^{n_H} (1-f_H)^{N-n_H} df_H = 1 \\
    \iff & P(n_H | N) = \binom{N}{n_H} \frac{\Gamma(n_H + 1) \Gamma(N-n_H+1)}{\Gamma(N+2)}\\
    & = \binom{N}{n_H} \frac{n_H! (N-n_H)!}{(N+1)!}\\
    & = \frac{n_H!(N-n_H)!}{(N+1)n_H!(N-n_H)!} = \frac{1}{N+1}
\end{align*}
so
$$P(f_H | n_H, N) = \frac{(N+1)!}{n_H!(N-n_H)!} f_H^{n_H} (1-f_H)^{N-n_H}$$

\vspace{1em}
The graph looks like this: \href{https://www.desmos.com/calculator/bjjlgnqg87}{www.desmos.com/calculator/bjjlgnqg87}

Now,
\begin{align*}
    \operatorname{E}[f_H] &= \int_{0}^{1} f_H P(f_H | n_H, N) df_H \\
    &= \frac{(N+1)!}{n_H!(N-n_H)!} \int_{0}^{1} f_H^{n_H+1}(1-f)^{N-n_H} df_H \\
    &= \frac{(N+1)!}{n_H!(N-n_H)!} \frac{(n_H+1)!(N-n_H)!}{(N+2)!} \\
    &= \frac{n_H+1}{N+2}
\end{align*}
Thus,
\begin{enumerate}
    \item[(a)] $\frac{1}{5}$
    \item[(b)] $\frac{3}{5}$
    \item[(c)] $\frac{4}{12} = \frac{1}{3}$
    \item[(d)] $\frac{30}{302} = \frac{15}{151}$
\end{enumerate}
\end{sol}
\bigskip

\begin{prob}{2.14}
    Prove Jensenâ€™s inequality.
\end{prob}
\begin{sol}
    We want to prove 
    $$
    \sum_{i=1}^{I} p_i f(x_i) \geq f(\sum_{i=1}^{I} p_i x_i) \quad \text{for} \, \sum p_i = 1 \quad \text{and $f$ convex.}
    $$
    Now,
    \begin{align*}
        f(\sum_{i=1}^{I} p_i x_i) &= f(p_1 x_1 + \sum_{i=1}^{I} p_i x_i) \\
        &\leq p_1 f(x_1) + [\sum_{i=2}^{I} p_i] [f(\frac{\sum_{i=2}^{I} p_i x_i}{\sum_{i=2}^{I} p_i})] \quad (\because \text{Definition of convex function}) \\
        &\leq p_1 f(x_1) + [\sum_{i=2}^{I} p_i] [\frac{p_2}{\sum_{i=2}^{I} p_i} f(x_2) + \frac{\sum_{i=3}^{I} p_i}{\sum_{i=2}^{I} p_i} f(\frac{\sum_{i=3}^{I} p_i x_i}{\sum_{i=3}^{I} p_i})] \\
        &= p_1 f(x_1) + p_2 f(x_2) + \frac{\sum_{i=3}^{I} p_i}{\sum_{i=2}^{I} p_i} f(\frac{\sum_{i=3}^{I} p_i x_i}{\sum_{i=3}^{I} p_i}) \\
        &= \dots \\
        &\leq p_1 f(x_1) + p_2 f(x_2) + p_3 f(x_3) + p_4 f(x_4) + \dots \\
        &= \sum_{i=1}^{I} p_i f(x_i) \quad \blacksquare
    \end{align*}
\end{sol}
\bigskip

\begin{prob}{2.16}
    \begin{enumerate}
        \item[(a)]Two ordinary dice with faces labelled 1,...,6 are
thrown. What is the probability distribution of the sum of the values? What is the probability distribution of the absolute difference
between the values?
\item[(b)] One hundred ordinary dice are thrown. What, roughly, is the probability distribution of the sum of the values? Sketch the probability
distribution and estimate its mean and standard deviation.
\item[(c)] How can two cubical dice be labelled using the numbers
${0, 1, 2, 3, 4, 5, 6}$ so that when the two dice are thrown the sum
has a uniform probability distribution over the integers $1 - 12$?
\item[(d)] Is there any way that one hundred dice could be labelled with integers such that the probability distribution of the sum is uniform?
    \end{enumerate}
\end{prob}
\begin{sol}
\begin{enumerate}
\item[(a)] let S = sum of values. \\[1em]
\begin{tabular}{|c|ccccccccccc|}
    \hline
    $s$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
    \hline
    $P(S=s)$ & $\frac{1}{36}$ & $\frac{1}{18}$ & $\frac{1}{12}$ & $\frac{1}{9}$ & $\frac{5}{36}$ & $\frac{1}{6}$ & $\frac{5}{36}$ & $\frac{1}{9}$ & $\frac{1}{12}$ & $\frac{1}{18}$ & $\frac{1}{36}$ \\
    \hline
\end{tabular} \\

Let D = difference of values \\[1em]
\begin{tabular}{|c|cccccc|}
    \hline
    $d$ & 0 & 1 & 2 & 3 & 4 & 5 \\
    \hline
    $P(D=d)$ & $\frac{1}{6}$ & $\frac{1}{3}$ & $\frac{2}{9}$ & $\frac{1}{6}$ & $\frac{1}{9}$ & $\frac{1}{18}$ \\
    \hline
\end{tabular}
\item[(b)] 
\begin{align*}
    \operatorname{E}[S_{100}] &= 100 \operatorname{E}[s] \\
    &= 350 \quad (\because \text{Independent})  \\
    \operatorname{Var}[S_{100}] &= 100\operatorname{Var}[s] \\
    &= 100 \left(\frac{91}{6} - \left(\frac{7}{2}\right)^2\right) = \frac{875}{3} \approx 292
\end{align*}
$P(S=s)$ vs $s$ graph will be bell-curve-like between 100 and 600 with mean 350.
\item[(c)] Die 1 with \{1, 2, 3, 4, 5, 6\} and die 2 with \{0, 0, 0, 6, 6, 6\}.
\item[(d)] Make $r$th die to be $\{0, 1, 2, 3, 4, 5\} \times 6^r$ so each combination will sum to a unique number. \\
This does not violate the CLT as variables are not identical and the Lindeberg Condition (no single variance dominate) is not satisfied.
\end{enumerate}
\end{sol}
\bigskip

\begin{prob}{2.17}
    If $q = 1 - p$ and $a = \ln p/q$, show that
\begin{equation}
    p = \frac{1}{1 + \exp(-a)}. \tag{2.50}
\end{equation}
Sketch this function and find its relationship to the hyperbolic tangent function $\tanh(u) = \frac{e^u - e^{-u}}{e^u + e^{-u}}$.

\medskip

\noindent It will be useful to be fluent in base-2 logarithms also. If $b = \log_2 p/q$, what is $p$ as a function of $b$?
\end{prob}
\begin{sol}
    \begin{align*}
        \frac{1}{1+\exp(-a)} &= \frac{1}{1 + \exp(-\ln \frac{p}{q})} = \frac{1}{1 + (\frac{p}{q})^{-1}} \\
        &= \frac{1}{1 + \frac{q}{p}} = \frac{p}{p + q} = p 
    \end{align*}
    Now, with tanh, 
    \begin{align*}
        p &= \frac{1}{1 + e^{-u}} \\
        &= \frac{1}{2} \frac{2e^{\frac{u}{2}}}{e^{\frac{u}{2}} + e^{-\frac{u}{2}}} \\
        &= \frac{1}{2} \left( \frac{e^{\frac{u}{2}} - e^{-\frac{u}{2}}}{e^{\frac{u}{2}} + e^{-\frac{u}{2}}} + 1 \right) \\
        &= \frac{1}{2} \tanh \left(\frac{u}{2}\right) + \frac{1}{2}
    \end{align*}
    For base 2,
    \begin{align*}
          & b = \log_2 \frac{p}{q} \\
        \iff & 2^b = \frac{p}{1-p} \\
        \iff & 2^b = \frac{p}{q} \\
        \iff & (2^b + 1)p = 2^b \\
        \iff & p = \frac{2^b}{2^b + 1} = \frac{1}{1 + 2^{-b}}
    \end{align*}
\end{sol}
\bigskip

\begin{prob}{2.18}
Let $x$ and $y$ be dependent random variables with $x$ a binary variable taking values in $\mathcal{A}_X = \{0, 1\}$. Use Bayes' theorem to show that the log posterior probability ratio for $x$ given $y$ is
\begin{equation}
\log \frac{P(x=1 | y)}{P(x=0 | y)} = \log \frac{P(y | x=1)}{P(y | x=0)} + \log \frac{P(x=1)}{P(x=0)}. \tag{2.51}
\end{equation}
\end{prob}
\begin{sol}
\begin{align*}
    \log \frac{P(x=1 | y)}{P(x=0 | y)} &= \log \frac{P(y | x=1) \cdot P(x=1) \cdot P(y)}{P(y | x=0)\cdot P(x=0)\cdot P(y)} \\
    &= \log \frac{P(y | x=1)}{P(y | x=0)} + \log \frac{P(x=1)}{P(x=0)}
\end{align*}
\end{sol}
\bigskip

\begin{prob}{2.19}
    Let $x, d_1$ and $d_2$ be random variables such that $d_1$ and $d_2$ are conditionally independent given a binary variable $x$. Use Bayes' theorem to show that the posterior probability ratio for $x$ given $\{d_i\}$ is
\begin{equation}
\frac{P(x=1 | \{d_i\})}{P(x=0 | \{d_i\})} = \frac{P(d_1 | x=1)}{P(d_1 | x=0)} \frac{P(d_2 | x=1)}{P(d_2 | x=0)} \frac{P(x=1)}{P(x=0)}. \tag{2.52}
\end{equation}
\end{prob}
\begin{sol}
    \begin{align*}
        \frac{P(x=1 | \{d_i\})}{P(x=0 | \{d_i\})} &=\frac{P(d_1, d_2 | x=1) P(x=1)}{P(d_1, d_2)} \cdot \frac{P(d_1, d_2)}{P(d_1, d_2 | x=0) P(x=0)} \\
        &= \frac{P(d_1 | x=1)}{P(d_1 | x=0)} \frac{P(d_2 | x=1)}{P(d_2 | x=0)} \frac{P(x=1)}{P(x=0)} \quad (\because d_1, d_2 \text{ independent})
    \end{align*}
\end{sol}
\bigskip

\begin{prob}{2.20}
    Consider a sphere of radius $r$ in an $N$-dimensional real space. Show that the fraction of the volume of the sphere that is in the surface shell lying at values of the radius between $r - \epsilon$ and $r$, where $0 < \epsilon < r$, is:
\begin{equation}
f = 1 - \left( 1 - \frac{\epsilon}{r} \right)^N. \tag{2.53}
\end{equation}
Evaluate $f$ for the cases $N = 2$, $N = 10$ and $N = 1000$, with (a) $\epsilon/r = 0.01$; (b) $\epsilon/r = 0.5$.

\end{prob}
\begin{sol}
    $V_{\text{sphere}} \propto r^N$ so with $a$ constant,\\
\begin{equation*}
    f = \frac{ar^N - a(r-\epsilon)^N}{ar^N} = 1 - (1 - \frac{\epsilon}{r})^N
\end{equation*}
\begin{enumerate}
    \item[(a)] 
\begin{tabular}{c|ccc}
    N & 2 & 100 & 1000 \\
    \hline
    f ($\frac{\epsilon}{r} = 0.01$) & 0.0199 & 0.0956 & 0.99996 \\
\end{tabular}
    \item[(b)] 
\begin{tabular}{c|ccc}
    N & 2 & 10 & 1000 \\
    \hline
    f ($\frac{\epsilon}{r} = 0.5$) & 0.75 & 0.999 & 1.0 \\
\end{tabular}
\end{enumerate}
\end{sol}
\bigskip


\begin{prob}{2.21}
    Let $p_a=0.1$, $p_b=0.2$, and $p_c=0.7$. Let $f(a)=10$, $f(b)=5$, and $f(c)=10/7$. What is $\mathcal{E}[f(x)]$? What is $\mathcal{E}[1/P(x)]$?
\end{prob}
\begin{sol}
    \begin{align*}
        \mathcal{E}\left[f(x)\right] &=  \sum_{x}P(x)f(x) \\
        &= 0.1 \cdot 10 + 0.2 \cdot 5 + 0.7 \cdot \frac{10}{7} \\
        &= 3 \\[10pt]
        \mathcal{E}\left[\frac{1}{P(x)}\right] &=  \sum_{x}P(x)\frac{1}{P(x)} \\
        &= 1 + 1 + 1 = 3
    \end{align*}
\end{sol}
\bigskip

\begin{prob}{2.22}
    For an arbitrary ensemble, what is $\mathcal{E}[1/P(x)]$?
\end{prob}
\begin{sol}
    \begin{align*}
        \mathcal{E}\left[\frac{1}{P(x)}\right] &=  \sum_{x}P(x)\frac{1}{P(x)} \\
        &= \sum_{x \in \mathcal{A}_X} 1 \\
        &= |x|
    \end{align*}
\end{sol}
\bigskip


\begin{prob}{2.23}
    Let $p_a=0.1$, $p_b=0.2$, and $p_c=0.7$. Let $g(a)=0$, $g(b)=1$, and $g(c)=0$. What is $\mathcal{E}[g(x)]$?
\end{prob}
\begin{sol}
    $$\mathcal{E}[g(x)] = 0.1 \cdot 0.1 + 0.2 \cdot 1 + 0.7 \cdot 0 = 0.2$$
\end{sol}
\bigskip

\begin{prob}{2.24}
    Let $p_a=0.1$, $p_b=0.2$, and $p_c=0.7$. What is the probability that $P(x) \in [0.15, 0.5]$? What is
\[
P \left( \left| \log \frac{P(x)}{0.2} \right| > 0.05 \right)?
\]
\end{prob}
\begin{sol}
    $x = b$ is the only possibility for $P(x) \in [0.15, 0.5]$ so 0.2. \\
    For the second part,
    \begin{align*}
        &\left| \log \frac{P(x)}{0.2} \right| > 0.05 \\
        &\iff \log \frac{P(x)}{0.2} < -0.05, \; 0.05 < \log \frac{P(x)}{0.2} \\
        &\iff P(x) < 2^{-0.05} \cdot 0.2, \; 2^{0.05} \cdot 0.2 < P(x) \\
        &\iff P(x) <  0.1932, \; 0.2071 < P(x) 
    \end{align*}
    so
    \begin{align*}
        P \left( \left| \log \frac{P(x)}{0.2} \right| > 0.05 \right) 
        &\iff p_a + p_b = 0.8
    \end{align*}
\end{sol}
\bigskip

\begin{prob}{2.25}
    Prove the assertion that $H(X) \leq \log(|\mathcal{A}_X|)$ with equality iff $p_i = 1/|\mathcal{A}_X|$ for all $i$. ($|\mathcal{A}_X|$ denotes the number of elements in the set $\mathcal{A}_X$.) [Hint: use Jensen's inequality (2.48); if your first attempt to use Jensen does not succeed, remember that Jensen involves both a random variable and a function, and you have quite a lot of freedom in choosing these; think about whether your chosen function $f$ should be convex or concave.]
\end{prob}
\begin{sol}
    \begin{align*}
        H(X) &= \operatorname{E}\left[\log \frac{1}{P(X)} \right] \\
        & \leq \log (\operatorname{E}\left[\log \frac{1}{P(X)} \right]) \\
        &= \log (|\mathcal{A}_X|) \quad (\because \text{concave})
    \end{align*}
    Equality when $\frac{1}{P(X)}$ is constant, i.e. $p_i$ constant, i.e. $p_i = \frac{1}{|\mathcal{A}_X|}$
\end{sol}
\bigskip

\begin{prob}{2.26}
    Prove that the relative entropy (equation (2.45)) satisfies $D_{\text{KL}}(P||Q) \geq 0$ (Gibbs' inequality) with equality only if $P = Q$.
\end{prob}
\begin{sol}
    \begin{align*}
    \sum_{x}P(X) \log \frac{P(X)}{Q(X)} &= \operatorname{E}\left[-\log \frac{Q(X)}{P(X)}\right] \\
    &\leq -\log \operatorname{E}\left[\frac{Q(X)}{P(X)}\right] \\
    &= -\log \sum_{x} Q(X) \\
    &= -\log 1 = 0
    \end{align*}
Equality iff $\frac{Q(X)}{P(X)}$ constant, \\
$\iff Q(X) = P(X) \quad (\because \sum_{x} Q(X) = \sum_{x} P(X) = 1)$
\end{sol}
\bigskip

\begin{prob}{2.27}
    Prove that the entropy is indeed decomposable as described in equations (2.43--2.44).
\end{prob}
\begin{sol}
    Let $p_x \sim p_y = \sum_{i=1}^{x} p_i$.
    \begin{align*}
        H(p) &= \sum_{i=1}^{I} p_i \log \frac{1}{p_i} \\
        &= \sum_{i=1}^{m} (p_1 \sim p_m) \frac{p_i}{p_1 \sim p_m} \log \frac{1}{p_i} + \sum_{i=m+1}^{I} (p_{m+1} \sim p_I) \frac{p_i}{p_{m+1} \sim p_I} \log \frac{1}{p_i} \\
        &= (p_1 \sim p_m) \sum_{i=1}^{m} \frac{p_i}{p_1 \sim p_m} \log \frac{p_1 \sim p_m}{p_i} + \sum_{i=1}^{m} (p_1 \sim p_m) \frac{p_i}{p_1 \sim p_m} \log \frac{1}{p_1 \sim p_m} \\
        &+ \sum_{i=m+1}^{I} \frac{(p_{m+1} \sim p_I)p_i}{p_{m+1} \sim p_I} \log \frac{p_{m+1} \sim p_I}{p_i} + \sum_{i=m+1}^{I}  \frac{(p_{m+1} \sim p_I)p_i}{p_{m+1} \sim p_I} \log \frac{1}{p_{m+1} \sim p_I} \\
        &= (p_1 \sim p_m) H(\frac{p_1}{p_1 \sim p_m}, \dots , \frac{p_m}{p_1 \sim p_m}) + (p_{m+1} \sim p_I) H(\frac{p_{m+1}}{p_{m+1} \sim p_I}, \dots , \frac{p_I}{p_{m+1} \sim p_I}) \\
        &+ H((p_1 \sim p_m), (p_{m+1} \sim p_I))
    \end{align*}
\end{sol}
\bigskip


\begin{prob}{2.28}
A random variable $x \in \{0, 1, 2, 3\}$ is selected by flipping a bent coin with bias $f$ to determine whether the outcome is in $\{0, 1\}$ or $\{2, 3\}$; then either flipping a second bent coin with bias $g$ or a third bent coin with bias $h$ respectively. Write down the probability distribution of $x$. Use the decomposability of the entropy (2.44) to find the entropy of $X$. [Notice how compact an expression is obtained if you make use of the binary entropy function $H_2(x)$, compared with writing out the four-term entropy explicitly.] Find the derivative of $H(X)$ with respect to $f$. [Hint: $dH_2(x)/dx = \log((1-x)/x)$.]
\end{prob}
\begin{sol}
    Probability distribution: \\[1em]
    \begin{tabular}{c|c|c|c|c}
        $x$ & $0$ & $1$ & $2$ & $3$ \\
        \hline
        $P(X = x)$ & $fg$ & $f(1-g)$ & $(1-f)h$ & $(1-f)(1-h)$ \\
    \end{tabular}
    Using the decomposability,
    $$H(X) = H_2(f) + fH_2(g) + (1-f)H_2(h)$$
    The derivative is,
    $$\frac{dH(X)}{df} = \log \frac{1-f}{f} + H_2(g) - H_2(h)$$
\end{sol}
\bigskip

\begin{prob}{2.29}
    An unbiased coin is flipped until one head is thrown. What is the entropy of the random variable $x \in \{1, 2, 3, \dots\}$, the number of flips? Repeat the calculation for the case of a biased coin with probability $f$ of coming up heads. [Hint: solve the problem both directly and by using the decomposability of the entropy (2.43).]
\end{prob}
\begin{sol}
    Solving directly, 
    \begin{align*}
        H(X) &= \frac{1}{2} \log 2 + \frac{1}{4} \log 4 + \frac{1}{8} \log 8 + \frac{1}{16} \log 16 + \dots \\
        &= (\frac{1}{2} + \frac{2}{4} + \frac{3}{8} + \frac{4}{16} + \dots) \log 2 \\
        &= \log 2 \sum_{i=1}^{\infty} \frac{i}{2^i} \\
        &= 2 \log 2 \quad (\because \sum_{i=1}^{\infty} ix^i = \frac{x}{(1-x)^2})\\
        &= 2
    \end{align*}
    With probability $f$,
    \begin{align*}
        H(X) &= f \log \frac{1}{f} + (1-f)f \log \frac{1}{(1-f)f} + \dots \\
        &= \sum_{i=1}^{\infty} -(1-f)^{i-1} f \log(f(1-f)^{i-1}) \\
        &= \sum_{i=1}^{\infty} \left\{ -(1-f)^{i-1}f \log f + (1-i)(1-f)^{i-1}f \log(1-f) \right\} \\
        &= \log \frac{1}{f} + \log (1-f) + f \log \frac{1}{1-f}
    \end{align*}
    Using decomposability,
    \begin{align*}
        & H(X) = H_2{f} + (1-f)H(X) \\
        \iff & H(X) = \frac{H_2(f)}{f}
    \end{align*}
    which is equal to the directly calculated answer.
\end{sol}
\bigskip

\begin{prob}{2.30}
    An urn contains $w$ white balls and $b$ black balls. Two balls are drawn, one after the other, without replacement. Prove that the probability that the first ball is white is equal to the probability that the second is white.
\end{prob}
\begin{sol}
    \begin{align}
        P(w_1) &= \frac{w}{w+b} \\
        P(w_2) &= P(w_2 \land w_1) + P(w_2 \land b_1) \\
        &= \frac{w-1}{w+b-1} \times \frac{w}{w+b} + \frac{w}{w+b-1} \times \frac{b}{w+b} \\
        &= \frac{w^2 - w + wb}{(w+b-1)(w+b)} \\
        &= \frac{w}{w+b}
    \end{align}
\end{sol}
\bigskip

\begin{prob}{2.31}
    A circular coin of diameter $a$ is thrown onto a square grid whose squares are $b \times b$. ($a < b$) What is the probability that the coin will lie entirely within one square? [Ans: $(1 - a/b)^2$]
\end{prob}
\begin{sol}
    The center of the coin must be at least $\frac{a}{2}$ away from the edge of the grid.
    Considering the ratio within each grid, $$\frac{(b-a)^2}{b^2} = (1-\frac{a}{b})^2$$ 
\end{sol}
\bigskip

\begin{prob}{2.32}
    \textbf{Buffon's needle.} A needle of length $a$ is thrown onto a plane covered with equally spaced parallel lines with separation $b$. What is the probability that the needle will cross a line? [Ans, if $a < b$: $2a/\pi b$] [Generalization -- Buffon's noodle: on average, a random curve of length $A$ is expected to intersect the lines $2A/\pi b$ times.]
\end{prob}
\begin{sol}
    Let the angle between the lines and the needle be $\theta$, then the chance of that needle crossing a line is $\frac{a \sin \theta}{b}$. \\ 
    Taking the average for the angle, the probability is 
    \begin{align*}
        \frac{1}{\pi} \int_{0}^{\pi} \frac{a \sin \theta}{b} d\theta &= \frac{1}{\pi}\left[-\frac{a}{b} \cos \theta \right]_0^\pi \\
        &= \frac{1}{\pi}\left(\frac{a}{b} - \left(- \frac{a}{b}\right)\right) \\
        &= \frac{2a}{\pi b}
    \end{align*}
\end{sol}
\bigskip

\begin{prob}{2.33}
    Two points are selected at random on a straight line segment of length 1. What is the probability that a triangle can be constructed out of the three resulting segments?
\end{prob}
\begin{sol}
    Assume $ 0 \leq x \leq y \leq x + y \leq 1$. \\
    Then, the lengths of the three segments will be $x$, $y-x$, $1-y$. \\
    From the triangle condition, 
    \begin{align*}
        &x + (y-x) > 1-y \\
        &(y-x) + (1-y) > x \\
        &(1-y) + x > y-x \\
        \iff & y > \frac{1}{2}, \; x < \frac{1}{2}, \; y < x + \frac{1}{2}
    \end{align*}
    We will now consider the graph where $y$ and $x$ can take. We have $x \geq 0$, $y \geq 0$, $y > x$, $x + y \leq 1$ as the assumption so the total area for a valid segmentation is $\frac{1}{2} \cdot 1 \cdot \frac{1}{2} = \frac{1}{4} $. \\
    The area within that further satisfies the above triangle condition is (by drawing the graph), $\frac{1}{2} \cdot \frac{1}{4} \cdot \frac{1}{2} = \frac{1}{16}$. \\
    Hence, the probability we require is $\frac{\frac{1}{16}}{\frac{1}{4}} = \frac{1}{4}$.
\end{sol}
\bigskip

\begin{prob}{2.34}
    An unbiased coin is flipped until one head is thrown. What is the expected number of tails and the expected number of heads?
    
    Fred, who doesn't know that the coin is unbiased, estimates the bias using $\hat{f} \equiv h/(h + t)$, where $h$ and $t$ are the numbers of heads and tails tossed. Compute and sketch the probability distribution of $\hat{f}$.
    
    N.B., this is a forward probability problem, a sampling theory problem, not an inference problem. Don't use Bayes' theorem.
\end{prob}
\begin{sol}
    \begin{align*}
    \operatorname{E}[H] &= 1 \\
    \operatorname{E}[T] &= \frac{1}{2} \cdot 0 + \frac{1}{4} \cdot 1 + \frac{1}{8} \cdot 2 + \frac{1}{16} \cdot 3 + \dots \\
    &= \sum_{i=1}^{\infty} \frac{i}{2^{i+1}} \\
    &= \frac{1}{2}\frac{\frac{1}{2}}{(1-\frac{1}{2})^2} = 1
    \end{align*}
    Now,
    $\hat{f} = \frac{1}{1+t}$ so \\

    \begin{tabular}{c|c|c|c|c|c}
        t & 0 & 1 & 2 & 3 & 4 \\
        \hline
        $\hat{f}$ & $1$ & $\frac{1}{2}$ & $\frac{1}{3}$ & $\frac{1}{4}$ & $\frac{1}{5}$
    \end{tabular}
    \\

    The graph will be a discrete version of $y = 2^{-\frac{1}{x} + 1}$.
\end{sol}
\bigskip

\begin{prob}{2.35}
    Fred rolls an unbiased six-sided die once per second, noting the occasions when the outcome is a six.
    \begin{enumerate}
        \item[(a)] What is the mean number of rolls from one six to the next six?
        \item[(b)] Between two rolls, the clock strikes one. What is the mean number of rolls until the next six?
        \item[(c)] Now think back before the clock struck. What is the mean number of rolls, going back in time, until the most recent six?
        \item[(d)] What is the mean number of rolls from the six before the clock struck to the next six?
        \item[(e)] Is your answer to (d) different from your answer to (a)? Explain.
    \end{enumerate}
\end{prob}
\begin{sol}
    \begin{enumerate}
        \item[(a)] \begin{align*}
        &1 \cdot \frac{1}{6} + 2 \cdot \frac{5}{6} \cdot \frac{1}{6} + 3 \cdot (\frac{5}{6})^2 \cdot \frac{1}{6} + \dots \\
        &= \sum_{i=1}^{\infty} i (\frac{5}{6})^{i-1} \cdot \frac{1}{6} \\
        &= \frac{1}{6} \cdot \frac{1}{(1-\frac{5}{6})^2} = 6 
        \end{align*}
        \item[(b)] Same as (a), so 6.
        \item[(c)] Same as (a), so 6. 
        \item[(d)] $6 + 6 - 1 = 11$
        \item[(e)] It is different because the probability that a one is struck between two rolls is higher when the gap between the two rolls is wider, so it is not equal to the distribution of the mean number of rolls.
    \end{enumerate}
\end{sol}
\bigskip

\begin{prob}{2.36}
    You meet Fred. Fred tells you he has two brothers, Alf and Bob.
    
    What is the probability that Fred is older than Bob?
    
    Fred tells you that he is older than Alf. Now, what is the probability that Fred is older than Bob? (That is, what is the conditional probability that $F > B$ given that $F > A$?)
\end{prob}
\begin{sol}
    There are six possible ordering, FAB, FBA, AFB, ABF, BAF, BFA. Fred is older than Bob for FAB, FBA, AFB so the answer is $\frac{1}{2}$. \\
    When it is given that Fred is older than Alf, the possible ordering reduces to FAB, FBA, BFA. Fred is older than Bob for FAB, FBA so now the answer is $\frac{2}{3}$.
\end{sol}
\bigskip

\begin{prob}{2.37}
    The inhabitants of an island tell the truth one third of the time. They lie with probability $2/3$.
On an occasion, after one of them made a statement, you ask another `was that statement true?' and he says `yes'.
    
    What is the probability that the statement was indeed true?
\end{prob}
\begin{sol}
    $$P(T|s) = \frac{P(T \land T)}{P(T \land T) + P(F \land F)} = \frac{\frac{1}{9}}{\frac{1}{9} + \frac{4}{9}} = \frac{1}{5}$$
\end{sol}
\bigskip

\begin{prob}{2.38}
    Compare two ways of computing the probability of error of the repetition code $R_3$, assuming a binary symmetric channel (you did this once for exercise 1.2 (p.7)) and confirm that they give the same answer. \\

\textbf{Binomial distribution method.} Add the probability that all three bits are flipped to the probability that exactly two bits are flipped.\\
    
    \textbf{Sum rule method.} Using the sum rule, compute the marginal probability that $\mathbf{r}$ takes on each of the eight possible values, $P(\mathbf{r})$. [$P(\mathbf{r}) = \sum_s P(s)P(\mathbf{r}|s)$.] Then compute the posterior probability of $s$ for each of the eight values of $\mathbf{r}$. [In fact, by symmetry, only two example cases $\mathbf{r} = (000)$ and $\mathbf{r} = (001)$ need be considered.] Notice that some of the inferred bits are better determined than others. From the posterior probability $P(s|\mathbf{r})$ you can read out the case-by-case error probability, the probability that the more probable hypothesis is not correct, $P(\text{error}|\mathbf{r})$. Find the average error probability using the sum rule,
    \begin{equation}
        P(\text{error}) = \sum_{\mathbf{r}} P(\mathbf{r})P(\text{error}|\mathbf{r}). \tag{2.55}
    \end{equation}
\end{prob}
\begin{sol}
    \textbf{Binomial distribution method.} $$\binom{3}{2} f^2 (1-f) + f^3 = 3f^2 - 3f^3 + f^3 = 3f^2 - 2f^3$$

    \textbf{Sum rule method.}
    $$P(r) = \sum_{s}P(s)P(r|s)$$
    When $r$ = 000,
    $$P(r) = \frac{1}{2}(1-f)^3 + \frac{1}{2}f^3$$
    $$P(\text{error}|r) = \frac{P(r|\text{error}) \cdot P(\text{error})}{P(r)} = \frac{f^3 \cdot \frac{1}{2}}{\frac{1}{2}(f^3 + (1-f)^3)} = \frac{f^3}{f^3 + (1-f)^3}$$
    When $r$ = 001, 
    $$P(r) = \frac{1}{2}(1-f)^2f + \frac{1}{2}(1-f)f^2 = \frac{1}{2} (1-f)f$$
    $$P(\text{error}|r) = \frac{P(r|\text{error}) \cdot P(\text{error})}{P(r)} = \frac{f^2(1-f) \cdot \frac{1}{2}}{\frac{1}{2}f(1-f)} = f$$
    Considering the symmetry for other $r$s, 
    \begin{align*}
        P(\text{error}) &= \sum_{\mathbf{r}} P(\mathbf{r})P(\text{error}|\mathbf{r}) \\
        &= 2 \times \frac{f^3}{f^3 + (1-f)^3} \times (\frac{1}{2}(1-f)^3 + \frac{1}{2}f^3) + 6 \times f \times \frac{1}{2}(1-f)f \\
        &= f^3 + 3f^2(1-f) \\
        &= 3f^2 - 2f^3
    \end{align*}
    which matches the result from the binomial distribution method.


\end{sol}
\bigskip

\begin{prob}{2.39}
    The frequency $p_n$ of the $n$th most frequent word in English is roughly approximated by
    \begin{equation}
        p_n \simeq 
        \begin{cases} 
        \frac{0.1}{n} & \text{for } n \in \{1, \dots, 12\,367\} \\
        0 & n > 12\,367.
        \end{cases} \tag{2.56}
    \end{equation}
    
    [This remarkable $1/n$ law is known as Zipf's law, and applies to the word frequencies of many languages (Zipf, 1949).] If we assume that English is generated by picking words at random according to this distribution, what is the entropy of English (per word)? 

\end{prob}
\begin{sol}
    \begin{align*}
        H(X) &= \sum_{n=1}^{12367} \frac{0.1}{n} \log \frac{n}{0.1} \\
        &= \sum_{n=1}^{12367} \frac{1}{10n} \log 10n \\
        &= \frac{1}{10} \sum_{n=1}^{12367} \frac{\log 10n}{n} \\
        &\approx \frac{1}{10} \int_{1}^{12367} \frac{\log_2 10n}{n} dn \\
        &= \frac{1}{10} \left[ \frac{(\log_2 10n)^2}{2 \ln 2} \right]_1^12367 \\
        &= \frac{\ln 2}{10} \left\{ \frac{(\log 123670)^2}{2} - \frac{(\log 10)^2}{2} \right\} \\
        &= 13.76 \ln 2 \\
        &= 9.5377
    \end{align*}
\end{sol}
\bigskip

% % template
% \begin{prob}{}
% \end{prob}
% \begin{sol}
% \end{sol}
% \bigskip

\end{document}

